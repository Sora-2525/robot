#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import torch
import pyaudio
import rclpy
from rclpy.node import Node
from faster_whisper import WhisperModel
from openai import OpenAI, APIError
import time  

import cv2
from sensor_msgs.msg import Image
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
from erasers_kachaka_common.tts import TTS  # TTS ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆROS Node å¿…é ˆï¼‰

class HumanTracker(Node):
    """äººç‰©è¿½è·¡ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        super().__init__("human_tracker")
        self.bridge = CvBridge()
        self.face_cascade = cv2.CascadeClassifier("/home/sora/opencv_data/haarcascades/haarcascade_frontalface_default.xml")
        self.cmd_pub = self.create_publisher(Twist, "/er_kachaka/manual_control/cmd_vel", 10)
        qos_profile = rclpy.qos.QoSProfile(depth=10)
        self.create_subscription(Image, "/er_kachaka/front_camera/image_raw", self.get_image, qos_profile)

    def get_image(self, msg):
        """ç”»åƒã‚’å–å¾—ã—äººç‰©è¿½è·¡ã‚’é–‹å§‹"""
        image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        faces = self.face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

        for (x, y, w, h) in faces:
            cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)
            self.track_person(x, y, w, h, image.shape[1])

    def track_person(self, x, y, w, h, frame_width):
        """é¡”ã®ä½ç½®ã‚’ã‚‚ã¨ã«ãƒ­ãƒœãƒƒãƒˆã‚’ç§»å‹•"""
        cmd = Twist()
        center_x = x + w / 2
        cmd.angular.z = 0.2 if center_x < frame_width / 2 - 30 else -0.2 if center_x > frame_width / 2 + 30 else 0.0
        self.cmd_pub.publish(cmd)

    def start_tracking(self):
        """è¿½è·¡ã‚’é–‹å§‹"""
        rclpy.spin(self)

def record_audio(seconds=5, rate=16000, chunk=1024):
    """éŸ³å£°éŒ²éŸ³"""
    print("ğŸ™ï¸ éŒ²éŸ³é–‹å§‹...")
    p = pyaudio.PyAudio()
    stream = p.open(format=pyaudio.paInt16, channels=1, rate=rate, input=True, frames_per_buffer=chunk)
    frames = []

    for _ in range(0, int(rate / chunk * seconds)):
        data = stream.read(chunk, exception_on_overflow=False)
        frames.append(np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0)

    stream.stop_stream()
    stream.close()
    p.terminate()
    print("ğŸ›‘ éŒ²éŸ³çµ‚äº†")
    return np.concatenate(frames, axis=0)

def transcribe_audio(audio_data):
    """Whisperã«ã‚ˆã‚‹æ–‡å­—èµ·ã“ã—"""
    print("ğŸ§  Whisperã«ã‚ˆã‚‹æ–‡å­—èµ·ã“ã—ä¸­...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = WhisperModel("large", device=device, compute_type="float32")

    segments, _ = model.transcribe(audio_data, beam_size=5, language="ja")
    return " ".join([seg.text for seg in segments])

def query_chatgpt(message):
    """ChatGPTã¨ã®å¯¾è©±"""
    print("ğŸ¤– ChatGPTã«å•ã„åˆã‚ã›ä¸­...")
    client = OpenAI()
    messages = [
        {"role": "system", "content": "ç«¯çš„ã«ç°¡ç´ ãªè¿”äº‹ã‚’ã™ã‚‹ãƒ­ãƒœãƒƒãƒˆ"},
        {"role": "user", "content": message}
    ]
    try:
        response = client.chat.completions.create(model="gpt-4o", messages=messages)
        return response.choices[0].message.content.strip()
    except APIError as e:
        print(f"âŒ ChatGPT API error: {e}")
        return "ã™ã¿ã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def main():
    rclpy.init()
    node = Node("tts_node_dummy")
    tts = TTS(node)
    tracker = HumanTracker()  # ğŸ”¥ è¿½è·¡ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–

    try:
        audio_data = record_audio(seconds=5)
        transcription = transcribe_audio(audio_data)

        print(f"ğŸ“ èªè­˜çµæœ: {transcription}")

        if len(transcription.strip()) < 3 or "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ" in transcription:
            print("âš ï¸ ç„¡åŠ¹ãªèªè­˜çµæœã®ãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
            return

        if "ã¤ã„ã¦ãã¦" in transcription:
            print("ğŸš¶â€â™‚ï¸ è¿½è·¡ãƒ¢ãƒ¼ãƒ‰ã‚’ç›´æ¥èµ·å‹•ï¼")
            tts.say("è¿½è·¡ã—ã¾ã™")
            tracker.start_tracking()  # ğŸ”¥ ã‚¯ãƒ©ã‚¹ã‚’ç›´æ¥å‘¼ã³å‡ºã™
            return
        
        if "ã‚«ãƒãƒ£ã‚«ã‚¹ã‚¿ãƒ¼ãƒˆ" in transcription:
            tts.say("ã­ãˆã€ã‚«ãƒãƒ£ã‚«")
            return

        reply = query_chatgpt(transcription)
        print(f"ğŸ¤– ChatGPTã®å¿œç­”: {reply}")

        print("ğŸ”Š TTS ç™ºè©±ä¸­...")
        tts.say(reply)
        print("âœ… å®Œäº†ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")

    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == "__main__":
    main()